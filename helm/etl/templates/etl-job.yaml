apiVersion: batch/v1
kind: CronJob
metadata:
  name: etl-cronjob
spec:
  suspend: {{ .Values.suspendCronjob }}
  schedule: {{ .Values.schedule | quote }}
  jobTemplate:
    spec:
      backoffLimit: 0
      template:
        metadata:
          {{- with .Values.podAnnotations }}
          annotations:
            {{- toYaml . | nindent 12 }}
          {{- end }}
          labels:
            app: gen3job
        spec:
          shareProcessNamespace: true
          affinity:
            nodeAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
                - weight: 100
                  preference:
                    matchExpressions:
                      - key: karpenter.sh/capacity-type
                        operator: In
                        values:
                          - on-demand
                - weight: 99
                  preference:
                    matchExpressions:
                      - key: eks.amazonaws.com/capacityType
                        operator: In
                        values:
                          - ONDEMAND
          volumes:
            {{- if .Values.legacySupport }}
            - name: config-volume
              secret:
                defaultMode: 420
                secretName: etl-secret  
            {{- end }}  
            - name: emptydir
              emptyDir: {}      
            - name: signal-volume
              emptyDir: {}
            - name: creds-volume
              secret:
                secretName: "peregrine-dbcreds"
            - name: etl-mapping
              configMap:
                name: etl-mapping
            - name: fence-usersync-yaml
              configMap:
                name: fence
                optional: true
                defaultMode: 0744
            - name: fence-useryaml
              configMap:
                name: useryaml
                defaultMode: 0744
          containers:
            - name: gen3-spark
              image: {{ .Values.image.spark.repository }}:{{ .Values.image.spark.tag }}
              ports:
              - containerPort: 22
              - containerPort: 9000
              - containerPort: 8030
              - containerPort: 8031
              - containerPort: 8032
              - containerPort: 7077
              readinessProbe:
                tcpSocket:
                  port: 9000
                periodSeconds: 10
              env:
              - name: DICTIONARY_URL
                valueFrom:
                  configMapKeyRef:
                    name: manifest-global
                    key: dictionary_url
              - name: HADOOP_URL
                value: hdfs://0.0.0.0:9000
              - name: HADOOP_HOST
                value: 0.0.0.0
              volumeMounts:
                - mountPath: /usr/share/pod
                  name: signal-volume
                  readOnly: true
              imagePullPolicy: {{ .Values.image.spark.pullPolicy }}
              resources:
                requests:
                  cpu: {{  .Values.resources.spark.requests.cpu }}
                  memory: {{ .Values.resources.spark.requests.memory }}
              command: ["/bin/bash" ]
              args: 
                - "-c"
                - |
                  python run_config.py
                  hdfs namenode -format
                  hdfs --daemon start namenode
                  hdfs --daemon start datanode
                  yarn --daemon start resourcemanager
                  yarn --daemon start nodemanager
                  hdfs dfsadmin -safemode leave
                  hdfs dfs -mkdir /result
                  hdfs dfs -mkdir /jars
                  hdfs dfs -mkdir /archive
                  /spark/sbin/start-all.sh
                  while true; do [ -f "/usr/share/pod/exit" ] && { exit_code=$(cat /usr/share/pod/exit); [[ "$exit_code" =~ ^[0-9]+$ ]] && echo "Exiting with exit code $exit_code" && exit $exit_code || exit 1; }; sleep 5; done
            - name: tube
              imagePullPolicy: IfNotPresent
              image: {{ .Values.image.tube.repository }}:{{ .Values.image.tube.tag }}
              ports:
                - containerPort: 80
              env:
                - name: DB_HOST
                  valueFrom:
                    secretKeyRef:
                      name: peregrine-dbcreds
                      key: host
                - name: DB_DATABASE
                  valueFrom:
                    secretKeyRef:
                      name: sheepdog-dbcreds
                      key: database
                - name: DB_USERNAME
                  valueFrom:
                    secretKeyRef:
                      name: sheepdog-dbcreds
                      key: username
                - name: DB_PASSWORD
                  valueFrom:
                    secretKeyRef:
                      name: sheepdog-dbcreds
                      key: password
                - name: DB_PORT
                  valueFrom:
                    secretKeyRef:
                      name: sheepdog-dbcreds
                      key: port
                - name: DICTIONARY_URL
                  valueFrom:
                    configMapKeyRef:
                      name: manifest-global
                      key: dictionary_url
                - name: HADOOP_URL
                  value: hdfs://localhost:9000
                - name: ES_URL
                  value: {{ .Values.esEndpoint  }}
                - name: HADOOP_HOST
                  value: localhost
                - name: HADOOP_CLIENT_OPTS
                  value: -Xmx1g
                - name: SPARK_EXECUTOR_MEMORY
                  value: 4g
                - name: SPARK_DRIVER_MEMORY
                  value: 6g
                - name: XDG_DATA_HOME
                  value: /tube
                - name: ETL_FORCED
                  value: {{ .Values.etlForced | quote }}
                - name: gen3Env
                  valueFrom:
                    configMapKeyRef:
                      name: manifest-global
                      key: hostname
                - name: slackWebHook
                  valueFrom:
                    configMapKeyRef:
                      name: global
                      key: slack_webhook
                      optional: true
              volumeMounts:
                {{- if .Values.legacySupport }}
                - mountPath: /tube/tube/settings.py
                  name: config-volume
                  subPath: settings.py
                {{- end }}
                - mountPath: /tube/gen3/tube
                  name: emptydir
                  defaultMode: 0755
                - mountPath: /usr/share/pod
                  name: signal-volume
                - name: "etl-mapping"
                  readOnly: true
                  mountPath: "/tube/gen3/tube/etlMapping.yaml"
                  subPath: "etlMapping.yaml"
                - name: fence-usersync-yaml
                  mountPath: /tube/gen3/tube/fence-user.yaml
                  subPath: user.yaml
                  defaultMode: 0755
                - name: fence-useryaml
                  mountPath: /tube/gen3/tube/useryaml-user.yaml
                  subPath: useryaml
                  defaultMode: 0755
              resources:
                requests:
                  cpu: {{  .Values.resources.tube.requests.cpu }}
                  memory: {{ .Values.resources.tube.requests.memory }}
              command: ["/bin/bash"]
              args:
                - "-c"
                - |
                  # Trap multiple signals to capture different exit scenarios
                  trap 'echo $? > /usr/share/pod/exit' EXIT SIGTERM SIGINT SIGHUP
                  if [ -f "/tube/gen3/tube/fence-user.yaml" ]; then
                    cp /tube/gen3/tube/fence-user.yaml /tube/gen3/tube/user.yaml
                    echo "Using Fence usersync user.yaml."
                  else
                    cp /tube/gen3/tube/useryaml-user.yaml /tube/gen3/tube/user.yaml
                    echo "Using Fence useryaml."
                  fi
                  while ! bash -c "echo >/dev/tcp/localhost/9000"; do
                    echo "Spark is not ready on port 9000... waiting for 10 seconds."
                    sleep 10
                  done

                  # Port 9000 is open, continue with the rest of the script
                  echo "Port 9000 is now open. Continuing with the script..."
                  if [[ $ETL_FORCED != "false" ]]; then
                    echo "python run_config.py && python run_etl.py --force"
                    python run_config.py && python run_etl.py --force
                  else
                    echo "python run_config.py && python run_etl.py"
                    python run_config.py && python run_etl.py
                  fi
                  exitcode=$?

                  # Immediately write exit code to the shared file signalling to spark to die.
                  echo "Writing exit code to signal file to let spark die"
                  echo "$exitcode" > /usr/share/pod/exit

                  echo "Exit code: $exitcode"
                  exit "$exitcode"
          restartPolicy: Never
