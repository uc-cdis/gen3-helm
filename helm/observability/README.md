# lgtma-chart

![Version: 0.1.3](https://img.shields.io/badge/Version-0.1.3-informational?style=flat-square) ![Type: application](https://img.shields.io/badge/Type-application-informational?style=flat-square) ![AppVersion: 1.0.0](https://img.shields.io/badge/AppVersion-1.0.0-informational?style=flat-square)

A Helm chart for deploying the LGTM stack with additional resources

## Requirements

| Repository | Name | Version |
|------------|------|---------|
| https://grafana.github.io/helm-charts | lgtm(lgtm-distributed) | 2.1.0 |

## Values

| Key | Type | Default | Description |
|-----|------|---------|-------------|
| lgtm.grafana | map | `{"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchExpressions":[{"key":"topology.kubernetes.io/zone","operator":"In","values":["us-east-1a"]}]}]}}},"alerting":{"contactpoints.yaml":{"secret":{"apiVersion":1,"contactPoints":[{"name":"slack","orgId":1,"receivers":[{"settings":{"group":"slack","summary":"{{ `{{ include \"default.message\" . }}` }}\n","url":"https://hooks.slack.com/services/XXXXXXXXXX"},"type":"Slack","uid":"first_uid"}]}]}},"rules.yaml":{"apiVersion":1,"groups":[{"folder":"Alerts","interval":"5m","name":"Alerts","orgId":1,"rules":[{"annotations":{"summary":"Alert: HTTP 500 errors detected in the environment: {{`{{ $labels.clusters }}`}}"},"condition":"A","data":[{"datasourceUid":"loki","model":{"datasource":{"type":"loki","uid":"loki"},"editorMode":"code","expr":"sum by (cluster) (count_over_time({cluster=~\".+\"} | json | http_status_code=\"500\" [1h])) > 0","hide":false,"intervalMs":1000,"maxDataPoints":43200,"queryType":"instant","refId":"A"},"queryType":"instant","refId":"A","relativeTimeRange":{"from":600,"to":0}}],"execErrState":"KeepLast","for":"5m","isPaused":false,"labels":{},"noDataState":"OK","notification_settings":{"receiver":"Slack"},"title":"HTTP 500 errors detected","uid":"edwb8zgcvq96oc"},{"annotations":{"description":"Error in usersync job detected in cluster {{`{{ $labels.clusters }}`}}, namespace {{`{{ $labels.namespace }}`}}.","summary":"Error Logs Detected in Usersync Job"},"condition":"A","data":[{"datasourceUid":"loki","model":{"datasource":{"type":"loki","uid":"loki"},"editorMode":"code","expr":"sum by (cluster, namespace) (count_over_time({ app=\"gen3job\", job_name=~\"usersync-.*\"} |= \"ERROR - could not revoke policies from user `N/A`\" [5m])) > 1","hide":false,"intervalMs":1000,"maxDataPoints":43200,"queryType":"instant","refId":"A"},"queryType":"instant","refId":"A","relativeTimeRange":{"from":600,"to":0}}],"execErrState":"KeepLast","for":"5m","isPaused":false,"labels":{},"noDataState":"OK","notification_settings":{"receiver":"Slack"},"title":"Error Logs Detected in Usersync Job","uid":"adwb9vhb7irr4b"},{"annotations":{"description":"Panic detected in app {{`{{ $labels.app }}`}} within cluster {{`{{ $labels.clusters }}`}}.","summary":"Hatchery panic"},"condition":"A","data":[{"datasourceUid":"loki","model":{"datasource":{"type":"loki","uid":"loki"},"editorMode":"code","expr":"sum by (cluster) (count_over_time({app=\"hatchery\"} |= \"panic\" [5m])) > 1","hide":false,"intervalMs":1000,"maxDataPoints":43200,"queryType":"instant","refId":"A"},"queryType":"instant","refId":"A","relativeTimeRange":{"from":600,"to":0}}],"execErrState":"KeepLast","for":"5m","isPaused":false,"labels":{},"noDataState":"OK","notification_settings":{"receiver":"Slack"},"title":"Hatchery panic in {{`{{ env.name }}`}}","uid":"ddwbc12l6wc8wf"},{"annotations":{"description":"Detected 431 HTTP status codes in the logs within the last 5 minutes.","summary":"Http status code 431"},"condition":"A","data":[{"datasourceUid":"loki","model":{"datasource":{"type":"loki","uid":"loki"},"editorMode":"code","expr":"sum(count_over_time({cluster=~\".+\"} | json | http_status_code=\"431\" [5m])) >= 2","hide":false,"intervalMs":1000,"maxDataPoints":43200,"queryType":"instant","refId":"A"},"queryType":"instant","refId":"A","relativeTimeRange":{"from":600,"to":0}}],"execErrState":"KeepLast","for":"5m","isPaused":false,"labels":{},"noDataState":"OK","notification_settings":{"receiver":"Slack"},"title":"Http status code 431","uid":"cdwbcbphz1zb4a"},{"annotations":{"description":"High number of info status logs detected in the indexd service in cluster {{`{{ $labels.clusters }}`}}.","summary":"Indexd is getting an excessive amount of traffic"},"condition":"A","data":[{"datasourceUid":"loki","model":{"datasource":{"type":"loki","uid":"loki"},"editorMode":"code","expr":"sum by (cluster) (count_over_time({cluster=~\".+\", app=\"indexd\", status=\"info\"} [5m])) > 50000","hide":false,"intervalMs":1000,"maxDataPoints":43200,"queryType":"instant","refId":"A"},"queryType":"instant","refId":"A","relativeTimeRange":{"from":600,"to":0}}],"execErrState":"KeepLast","for":"5m","isPaused":false,"labels":{},"noDataState":"OK","notification_settings":{"receiver":"Slack"},"title":"Indexd is getting an excessive amount of traffic","uid":"bdwbck1lgwdfka"},{"annotations":{"description":"More than 10 errors detected in the karpenter namespace in cluster {{`{{ $labels.clusters }}`}} related to providerRef not found.","summary":"Karpenter Resource Mismatch"},"condition":"A","data":[{"datasourceUid":"loki","model":{"datasource":{"type":"loki","uid":"loki"},"editorMode":"code","expr":"sum by (cluster) (count_over_time({namespace=\"karpenter\", cluster=~\".+\"} |= \"ERROR\" |= \"not found\" |= \"getting providerRef\" [5m])) > 10\n","hide":false,"intervalMs":1000,"maxDataPoints":43200,"queryType":"instant","refId":"A"},"queryType":"instant","refId":"A","relativeTimeRange":{"from":600,"to":0}}],"execErrState":"KeepLast","for":"5m","isPaused":false,"labels":{},"noDataState":"OK","notification_settings":{"receiver":"Slack"},"title":"Karpenter Resource Mismatch","uid":"fdwbe5t439zpcd"},{"annotations":{"description":"More than 1000 \"limiting requests, excess\" errors detected in service {{`{{ $labels.app }}`}} (cluster: {{`{{ $labels.clusters }}`}}) within the last 5 minutes.","summary":"Nginx is logging excessive \" limiting requests, excess:\""},"condition":"A","data":[{"datasourceUid":"loki","model":{"datasource":{"type":"loki","uid":"loki"},"editorMode":"code","expr":"sum by (app, cluster) (count_over_time({app=~\".+\", cluster=~\".+\"} |= \"status:error\" |= \"limiting requests, excess:\" [5m])) > 1000","hide":false,"intervalMs":1000,"maxDataPoints":43200,"queryType":"instant","refId":"A"},"queryType":"instant","refId":"A","relativeTimeRange":{"from":600,"to":0}}],"execErrState":"KeepLast","for":"5m","isPaused":false,"labels":{},"noDataState":"OK","notification_settings":{"receiver":"Slack"},"title":"Nginx is logging excessive \" limiting requests, excess:\"","uid":"fdwbeuftc7400c"}]}]}},"dashboardProviders":{"dashboardproviders.yaml":{"apiVersion":1,"providers":[{"disableDeletion":true,"editable":true,"folder":"Kubernetes","name":"grafana-dashboards-kubernetes","options":{"path":"/var/lib/grafana/dashboards/grafana-dashboards-kubernetes"},"orgId":1,"type":"file"}]}},"dashboards":{"grafana-dashboards-kubernetes":{"k8s-system-api-server":{"token":"","url":"https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-system-api-server.json"},"k8s-system-coredns":{"token":"","url":"https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-system-coredns.json"},"k8s-views-global":{"token":"","url":"https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-views-global.json"},"k8s-views-namespaces":{"token":"","url":"https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-views-namespaces.json"},"k8s-views-nodes":{"token":"","url":"https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-views-nodes.json"},"k8s-views-pods":{"token":"","url":"https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-views-pods.json"}}},"downloadDashboardsImage":{"registry":"quay.io/curl","repository":"curl","tag":"8.8.0"},"enabled":true,"env":{"GF_SERVER_ROOT_URL":"https://grafana.example.com"},"envFromSecret":null,"grafana.ini":{"auth.okta":{"allow_sign_up":true,"auto_login":true,"enabled":true,"icon":"okta"},"feature_toggles":{"enable":"ssoSettingsAPI transformationsVariableSupport","ssoSettingsApi":true,"transformationsVariableSupport":true},"log":{"level":"debug"},"server":{"domain":"grafana.example.com","root_url":"https://%(domain)s/"},"users":{"auto_assign_org_role":"Editor"}},"image":{"pullPolicy":"Always","registry":"quay.io/cdis","repository":"grafana","tag":"master"},"ingress":{"annotations":{},"enabled":true,"hosts":["grafana.example.com"],"ingressClassName":"alb","tls":[{"secretName":null}]},"initChownData":{"image":{"registry":"quay.io/cdis","repository":"busybox","tag":"1.32.0"}},"persistence":{"enabled":true}}` | Grafana configuration. |
| lgtm.grafana."grafana.ini"."auth.okta" | map | `{"allow_sign_up":true,"auto_login":true,"enabled":true,"icon":"okta"}` | Okta authentication settings in Grafana. |
| lgtm.grafana."grafana.ini"."auth.okta".allow_sign_up | bool | `true` | Allow users to sign up automatically using Okta. |
| lgtm.grafana."grafana.ini"."auth.okta".auto_login | bool | `true` | Automatically log in users using Okta when visiting Grafana. |
| lgtm.grafana."grafana.ini"."auth.okta".enabled | bool | `true` | Enable or disable Okta authentication. |
| lgtm.grafana."grafana.ini"."auth.okta".icon | string | `"okta"` | Icon used for Okta in the Grafana UI. |
| lgtm.grafana."grafana.ini".feature_toggles | map | `{"enable":"ssoSettingsAPI transformationsVariableSupport","ssoSettingsApi":true,"transformationsVariableSupport":true}` | Feature toggles in Grafana. |
| lgtm.grafana."grafana.ini".feature_toggles.enable | list | `"ssoSettingsAPI transformationsVariableSupport"` | Features to be enabled in Grafana. |
| lgtm.grafana."grafana.ini".feature_toggles.ssoSettingsApi | bool | `true` | Enable Single Sign-On (SSO) settings API. |
| lgtm.grafana."grafana.ini".feature_toggles.transformationsVariableSupport | bool | `true` | Enable support for transformations using variables in Grafana. |
| lgtm.grafana."grafana.ini".log | map | `{"level":"debug"}` | Logging configuration in Grafana. |
| lgtm.grafana."grafana.ini".log.level | string | `"debug"` | Logging level for Grafana. Options: debug, info, warn, error. |
| lgtm.grafana."grafana.ini".server | map | `{"domain":"grafana.example.com","root_url":"https://%(domain)s/"}` | Server configuration in Grafana. |
| lgtm.grafana."grafana.ini".server.domain | string | `"grafana.example.com"` | Domain name for the Grafana server. |
| lgtm.grafana."grafana.ini".server.root_url | string | `"https://%(domain)s/"` | Root URL for Grafana, using the domain name. |
| lgtm.grafana."grafana.ini".users | map | `{"auto_assign_org_role":"Editor"}` | User configuration settings in Grafana. |
| lgtm.grafana."grafana.ini".users.auto_assign_org_role | string | `"Editor"` | Auto-assign the specified role to new users upon login. Options: Viewer, Editor, Admin. |
| lgtm.grafana.affinity | map | `{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchExpressions":[{"key":"topology.kubernetes.io/zone","operator":"In","values":["us-east-1a"]}]}]}}}` | Affinity rules for scheduling Grafana pods. |
| lgtm.grafana.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0] | string | `{"key":"topology.kubernetes.io/zone","operator":"In","values":["us-east-1a"]}` | Node label key for affinity. Ensures pods are scheduled on nodes in the specified zone. |
| lgtm.grafana.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0].operator | string | `"In"` | Operator to apply to the node selector. 'In' means the node must match one of the values. |
| lgtm.grafana.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0].values | list | `["us-east-1a"]` | List of values for the node selector, representing allowed zones. |
| lgtm.grafana.alerting | map | `{"contactpoints.yaml":{"secret":{"apiVersion":1,"contactPoints":[{"name":"slack","orgId":1,"receivers":[{"settings":{"group":"slack","summary":"{{ `{{ include \"default.message\" . }}` }}\n","url":"https://hooks.slack.com/services/XXXXXXXXXX"},"type":"Slack","uid":"first_uid"}]}]}},"rules.yaml":{"apiVersion":1,"groups":[{"folder":"Alerts","interval":"5m","name":"Alerts","orgId":1,"rules":[{"annotations":{"summary":"Alert: HTTP 500 errors detected in the environment: {{`{{ $labels.clusters }}`}}"},"condition":"A","data":[{"datasourceUid":"loki","model":{"datasource":{"type":"loki","uid":"loki"},"editorMode":"code","expr":"sum by (cluster) (count_over_time({cluster=~\".+\"} | json | http_status_code=\"500\" [1h])) > 0","hide":false,"intervalMs":1000,"maxDataPoints":43200,"queryType":"instant","refId":"A"},"queryType":"instant","refId":"A","relativeTimeRange":{"from":600,"to":0}}],"execErrState":"KeepLast","for":"5m","isPaused":false,"labels":{},"noDataState":"OK","notification_settings":{"receiver":"Slack"},"title":"HTTP 500 errors detected","uid":"edwb8zgcvq96oc"},{"annotations":{"description":"Error in usersync job detected in cluster {{`{{ $labels.clusters }}`}}, namespace {{`{{ $labels.namespace }}`}}.","summary":"Error Logs Detected in Usersync Job"},"condition":"A","data":[{"datasourceUid":"loki","model":{"datasource":{"type":"loki","uid":"loki"},"editorMode":"code","expr":"sum by (cluster, namespace) (count_over_time({ app=\"gen3job\", job_name=~\"usersync-.*\"} |= \"ERROR - could not revoke policies from user `N/A`\" [5m])) > 1","hide":false,"intervalMs":1000,"maxDataPoints":43200,"queryType":"instant","refId":"A"},"queryType":"instant","refId":"A","relativeTimeRange":{"from":600,"to":0}}],"execErrState":"KeepLast","for":"5m","isPaused":false,"labels":{},"noDataState":"OK","notification_settings":{"receiver":"Slack"},"title":"Error Logs Detected in Usersync Job","uid":"adwb9vhb7irr4b"},{"annotations":{"description":"Panic detected in app {{`{{ $labels.app }}`}} within cluster {{`{{ $labels.clusters }}`}}.","summary":"Hatchery panic"},"condition":"A","data":[{"datasourceUid":"loki","model":{"datasource":{"type":"loki","uid":"loki"},"editorMode":"code","expr":"sum by (cluster) (count_over_time({app=\"hatchery\"} |= \"panic\" [5m])) > 1","hide":false,"intervalMs":1000,"maxDataPoints":43200,"queryType":"instant","refId":"A"},"queryType":"instant","refId":"A","relativeTimeRange":{"from":600,"to":0}}],"execErrState":"KeepLast","for":"5m","isPaused":false,"labels":{},"noDataState":"OK","notification_settings":{"receiver":"Slack"},"title":"Hatchery panic in {{`{{ env.name }}`}}","uid":"ddwbc12l6wc8wf"},{"annotations":{"description":"Detected 431 HTTP status codes in the logs within the last 5 minutes.","summary":"Http status code 431"},"condition":"A","data":[{"datasourceUid":"loki","model":{"datasource":{"type":"loki","uid":"loki"},"editorMode":"code","expr":"sum(count_over_time({cluster=~\".+\"} | json | http_status_code=\"431\" [5m])) >= 2","hide":false,"intervalMs":1000,"maxDataPoints":43200,"queryType":"instant","refId":"A"},"queryType":"instant","refId":"A","relativeTimeRange":{"from":600,"to":0}}],"execErrState":"KeepLast","for":"5m","isPaused":false,"labels":{},"noDataState":"OK","notification_settings":{"receiver":"Slack"},"title":"Http status code 431","uid":"cdwbcbphz1zb4a"},{"annotations":{"description":"High number of info status logs detected in the indexd service in cluster {{`{{ $labels.clusters }}`}}.","summary":"Indexd is getting an excessive amount of traffic"},"condition":"A","data":[{"datasourceUid":"loki","model":{"datasource":{"type":"loki","uid":"loki"},"editorMode":"code","expr":"sum by (cluster) (count_over_time({cluster=~\".+\", app=\"indexd\", status=\"info\"} [5m])) > 50000","hide":false,"intervalMs":1000,"maxDataPoints":43200,"queryType":"instant","refId":"A"},"queryType":"instant","refId":"A","relativeTimeRange":{"from":600,"to":0}}],"execErrState":"KeepLast","for":"5m","isPaused":false,"labels":{},"noDataState":"OK","notification_settings":{"receiver":"Slack"},"title":"Indexd is getting an excessive amount of traffic","uid":"bdwbck1lgwdfka"},{"annotations":{"description":"More than 10 errors detected in the karpenter namespace in cluster {{`{{ $labels.clusters }}`}} related to providerRef not found.","summary":"Karpenter Resource Mismatch"},"condition":"A","data":[{"datasourceUid":"loki","model":{"datasource":{"type":"loki","uid":"loki"},"editorMode":"code","expr":"sum by (cluster) (count_over_time({namespace=\"karpenter\", cluster=~\".+\"} |= \"ERROR\" |= \"not found\" |= \"getting providerRef\" [5m])) > 10\n","hide":false,"intervalMs":1000,"maxDataPoints":43200,"queryType":"instant","refId":"A"},"queryType":"instant","refId":"A","relativeTimeRange":{"from":600,"to":0}}],"execErrState":"KeepLast","for":"5m","isPaused":false,"labels":{},"noDataState":"OK","notification_settings":{"receiver":"Slack"},"title":"Karpenter Resource Mismatch","uid":"fdwbe5t439zpcd"},{"annotations":{"description":"More than 1000 \"limiting requests, excess\" errors detected in service {{`{{ $labels.app }}`}} (cluster: {{`{{ $labels.clusters }}`}}) within the last 5 minutes.","summary":"Nginx is logging excessive \" limiting requests, excess:\""},"condition":"A","data":[{"datasourceUid":"loki","model":{"datasource":{"type":"loki","uid":"loki"},"editorMode":"code","expr":"sum by (app, cluster) (count_over_time({app=~\".+\", cluster=~\".+\"} |= \"status:error\" |= \"limiting requests, excess:\" [5m])) > 1000","hide":false,"intervalMs":1000,"maxDataPoints":43200,"queryType":"instant","refId":"A"},"queryType":"instant","refId":"A","relativeTimeRange":{"from":600,"to":0}}],"execErrState":"KeepLast","for":"5m","isPaused":false,"labels":{},"noDataState":"OK","notification_settings":{"receiver":"Slack"},"title":"Nginx is logging excessive \" limiting requests, excess:\"","uid":"fdwbeuftc7400c"}]}]}}` | Gen3 built-in alerting configuration in Grafana. |
| lgtm.grafana.alerting."rules.yaml" | string | `{"apiVersion":1,"groups":[{"folder":"Alerts","interval":"5m","name":"Alerts","orgId":1,"rules":[{"annotations":{"summary":"Alert: HTTP 500 errors detected in the environment: {{`{{ $labels.clusters }}`}}"},"condition":"A","data":[{"datasourceUid":"loki","model":{"datasource":{"type":"loki","uid":"loki"},"editorMode":"code","expr":"sum by (cluster) (count_over_time({cluster=~\".+\"} | json | http_status_code=\"500\" [1h])) > 0","hide":false,"intervalMs":1000,"maxDataPoints":43200,"queryType":"instant","refId":"A"},"queryType":"instant","refId":"A","relativeTimeRange":{"from":600,"to":0}}],"execErrState":"KeepLast","for":"5m","isPaused":false,"labels":{},"noDataState":"OK","notification_settings":{"receiver":"Slack"},"title":"HTTP 500 errors detected","uid":"edwb8zgcvq96oc"},{"annotations":{"description":"Error in usersync job detected in cluster {{`{{ $labels.clusters }}`}}, namespace {{`{{ $labels.namespace }}`}}.","summary":"Error Logs Detected in Usersync Job"},"condition":"A","data":[{"datasourceUid":"loki","model":{"datasource":{"type":"loki","uid":"loki"},"editorMode":"code","expr":"sum by (cluster, namespace) (count_over_time({ app=\"gen3job\", job_name=~\"usersync-.*\"} |= \"ERROR - could not revoke policies from user `N/A`\" [5m])) > 1","hide":false,"intervalMs":1000,"maxDataPoints":43200,"queryType":"instant","refId":"A"},"queryType":"instant","refId":"A","relativeTimeRange":{"from":600,"to":0}}],"execErrState":"KeepLast","for":"5m","isPaused":false,"labels":{},"noDataState":"OK","notification_settings":{"receiver":"Slack"},"title":"Error Logs Detected in Usersync Job","uid":"adwb9vhb7irr4b"},{"annotations":{"description":"Panic detected in app {{`{{ $labels.app }}`}} within cluster {{`{{ $labels.clusters }}`}}.","summary":"Hatchery panic"},"condition":"A","data":[{"datasourceUid":"loki","model":{"datasource":{"type":"loki","uid":"loki"},"editorMode":"code","expr":"sum by (cluster) (count_over_time({app=\"hatchery\"} |= \"panic\" [5m])) > 1","hide":false,"intervalMs":1000,"maxDataPoints":43200,"queryType":"instant","refId":"A"},"queryType":"instant","refId":"A","relativeTimeRange":{"from":600,"to":0}}],"execErrState":"KeepLast","for":"5m","isPaused":false,"labels":{},"noDataState":"OK","notification_settings":{"receiver":"Slack"},"title":"Hatchery panic in {{`{{ env.name }}`}}","uid":"ddwbc12l6wc8wf"},{"annotations":{"description":"Detected 431 HTTP status codes in the logs within the last 5 minutes.","summary":"Http status code 431"},"condition":"A","data":[{"datasourceUid":"loki","model":{"datasource":{"type":"loki","uid":"loki"},"editorMode":"code","expr":"sum(count_over_time({cluster=~\".+\"} | json | http_status_code=\"431\" [5m])) >= 2","hide":false,"intervalMs":1000,"maxDataPoints":43200,"queryType":"instant","refId":"A"},"queryType":"instant","refId":"A","relativeTimeRange":{"from":600,"to":0}}],"execErrState":"KeepLast","for":"5m","isPaused":false,"labels":{},"noDataState":"OK","notification_settings":{"receiver":"Slack"},"title":"Http status code 431","uid":"cdwbcbphz1zb4a"},{"annotations":{"description":"High number of info status logs detected in the indexd service in cluster {{`{{ $labels.clusters }}`}}.","summary":"Indexd is getting an excessive amount of traffic"},"condition":"A","data":[{"datasourceUid":"loki","model":{"datasource":{"type":"loki","uid":"loki"},"editorMode":"code","expr":"sum by (cluster) (count_over_time({cluster=~\".+\", app=\"indexd\", status=\"info\"} [5m])) > 50000","hide":false,"intervalMs":1000,"maxDataPoints":43200,"queryType":"instant","refId":"A"},"queryType":"instant","refId":"A","relativeTimeRange":{"from":600,"to":0}}],"execErrState":"KeepLast","for":"5m","isPaused":false,"labels":{},"noDataState":"OK","notification_settings":{"receiver":"Slack"},"title":"Indexd is getting an excessive amount of traffic","uid":"bdwbck1lgwdfka"},{"annotations":{"description":"More than 10 errors detected in the karpenter namespace in cluster {{`{{ $labels.clusters }}`}} related to providerRef not found.","summary":"Karpenter Resource Mismatch"},"condition":"A","data":[{"datasourceUid":"loki","model":{"datasource":{"type":"loki","uid":"loki"},"editorMode":"code","expr":"sum by (cluster) (count_over_time({namespace=\"karpenter\", cluster=~\".+\"} |= \"ERROR\" |= \"not found\" |= \"getting providerRef\" [5m])) > 10\n","hide":false,"intervalMs":1000,"maxDataPoints":43200,"queryType":"instant","refId":"A"},"queryType":"instant","refId":"A","relativeTimeRange":{"from":600,"to":0}}],"execErrState":"KeepLast","for":"5m","isPaused":false,"labels":{},"noDataState":"OK","notification_settings":{"receiver":"Slack"},"title":"Karpenter Resource Mismatch","uid":"fdwbe5t439zpcd"},{"annotations":{"description":"More than 1000 \"limiting requests, excess\" errors detected in service {{`{{ $labels.app }}`}} (cluster: {{`{{ $labels.clusters }}`}}) within the last 5 minutes.","summary":"Nginx is logging excessive \" limiting requests, excess:\""},"condition":"A","data":[{"datasourceUid":"loki","model":{"datasource":{"type":"loki","uid":"loki"},"editorMode":"code","expr":"sum by (app, cluster) (count_over_time({app=~\".+\", cluster=~\".+\"} |= \"status:error\" |= \"limiting requests, excess:\" [5m])) > 1000","hide":false,"intervalMs":1000,"maxDataPoints":43200,"queryType":"instant","refId":"A"},"queryType":"instant","refId":"A","relativeTimeRange":{"from":600,"to":0}}],"execErrState":"KeepLast","for":"5m","isPaused":false,"labels":{},"noDataState":"OK","notification_settings":{"receiver":"Slack"},"title":"Nginx is logging excessive \" limiting requests, excess:\"","uid":"fdwbeuftc7400c"}]}]}` | Alerting rules configuration file. |
| lgtm.grafana.alerting."rules.yaml".apiVersion | int | `1` | API version for the alerting rules configuration. |
| lgtm.grafana.alerting."rules.yaml".groups | list | `[{"folder":"Alerts","interval":"5m","name":"Alerts","orgId":1,"rules":[{"annotations":{"summary":"Alert: HTTP 500 errors detected in the environment: {{`{{ $labels.clusters }}`}}"},"condition":"A","data":[{"datasourceUid":"loki","model":{"datasource":{"type":"loki","uid":"loki"},"editorMode":"code","expr":"sum by (cluster) (count_over_time({cluster=~\".+\"} | json | http_status_code=\"500\" [1h])) > 0","hide":false,"intervalMs":1000,"maxDataPoints":43200,"queryType":"instant","refId":"A"},"queryType":"instant","refId":"A","relativeTimeRange":{"from":600,"to":0}}],"execErrState":"KeepLast","for":"5m","isPaused":false,"labels":{},"noDataState":"OK","notification_settings":{"receiver":"Slack"},"title":"HTTP 500 errors detected","uid":"edwb8zgcvq96oc"},{"annotations":{"description":"Error in usersync job detected in cluster {{`{{ $labels.clusters }}`}}, namespace {{`{{ $labels.namespace }}`}}.","summary":"Error Logs Detected in Usersync Job"},"condition":"A","data":[{"datasourceUid":"loki","model":{"datasource":{"type":"loki","uid":"loki"},"editorMode":"code","expr":"sum by (cluster, namespace) (count_over_time({ app=\"gen3job\", job_name=~\"usersync-.*\"} |= \"ERROR - could not revoke policies from user `N/A`\" [5m])) > 1","hide":false,"intervalMs":1000,"maxDataPoints":43200,"queryType":"instant","refId":"A"},"queryType":"instant","refId":"A","relativeTimeRange":{"from":600,"to":0}}],"execErrState":"KeepLast","for":"5m","isPaused":false,"labels":{},"noDataState":"OK","notification_settings":{"receiver":"Slack"},"title":"Error Logs Detected in Usersync Job","uid":"adwb9vhb7irr4b"},{"annotations":{"description":"Panic detected in app {{`{{ $labels.app }}`}} within cluster {{`{{ $labels.clusters }}`}}.","summary":"Hatchery panic"},"condition":"A","data":[{"datasourceUid":"loki","model":{"datasource":{"type":"loki","uid":"loki"},"editorMode":"code","expr":"sum by (cluster) (count_over_time({app=\"hatchery\"} |= \"panic\" [5m])) > 1","hide":false,"intervalMs":1000,"maxDataPoints":43200,"queryType":"instant","refId":"A"},"queryType":"instant","refId":"A","relativeTimeRange":{"from":600,"to":0}}],"execErrState":"KeepLast","for":"5m","isPaused":false,"labels":{},"noDataState":"OK","notification_settings":{"receiver":"Slack"},"title":"Hatchery panic in {{`{{ env.name }}`}}","uid":"ddwbc12l6wc8wf"},{"annotations":{"description":"Detected 431 HTTP status codes in the logs within the last 5 minutes.","summary":"Http status code 431"},"condition":"A","data":[{"datasourceUid":"loki","model":{"datasource":{"type":"loki","uid":"loki"},"editorMode":"code","expr":"sum(count_over_time({cluster=~\".+\"} | json | http_status_code=\"431\" [5m])) >= 2","hide":false,"intervalMs":1000,"maxDataPoints":43200,"queryType":"instant","refId":"A"},"queryType":"instant","refId":"A","relativeTimeRange":{"from":600,"to":0}}],"execErrState":"KeepLast","for":"5m","isPaused":false,"labels":{},"noDataState":"OK","notification_settings":{"receiver":"Slack"},"title":"Http status code 431","uid":"cdwbcbphz1zb4a"},{"annotations":{"description":"High number of info status logs detected in the indexd service in cluster {{`{{ $labels.clusters }}`}}.","summary":"Indexd is getting an excessive amount of traffic"},"condition":"A","data":[{"datasourceUid":"loki","model":{"datasource":{"type":"loki","uid":"loki"},"editorMode":"code","expr":"sum by (cluster) (count_over_time({cluster=~\".+\", app=\"indexd\", status=\"info\"} [5m])) > 50000","hide":false,"intervalMs":1000,"maxDataPoints":43200,"queryType":"instant","refId":"A"},"queryType":"instant","refId":"A","relativeTimeRange":{"from":600,"to":0}}],"execErrState":"KeepLast","for":"5m","isPaused":false,"labels":{},"noDataState":"OK","notification_settings":{"receiver":"Slack"},"title":"Indexd is getting an excessive amount of traffic","uid":"bdwbck1lgwdfka"},{"annotations":{"description":"More than 10 errors detected in the karpenter namespace in cluster {{`{{ $labels.clusters }}`}} related to providerRef not found.","summary":"Karpenter Resource Mismatch"},"condition":"A","data":[{"datasourceUid":"loki","model":{"datasource":{"type":"loki","uid":"loki"},"editorMode":"code","expr":"sum by (cluster) (count_over_time({namespace=\"karpenter\", cluster=~\".+\"} |= \"ERROR\" |= \"not found\" |= \"getting providerRef\" [5m])) > 10\n","hide":false,"intervalMs":1000,"maxDataPoints":43200,"queryType":"instant","refId":"A"},"queryType":"instant","refId":"A","relativeTimeRange":{"from":600,"to":0}}],"execErrState":"KeepLast","for":"5m","isPaused":false,"labels":{},"noDataState":"OK","notification_settings":{"receiver":"Slack"},"title":"Karpenter Resource Mismatch","uid":"fdwbe5t439zpcd"},{"annotations":{"description":"More than 1000 \"limiting requests, excess\" errors detected in service {{`{{ $labels.app }}`}} (cluster: {{`{{ $labels.clusters }}`}}) within the last 5 minutes.","summary":"Nginx is logging excessive \" limiting requests, excess:\""},"condition":"A","data":[{"datasourceUid":"loki","model":{"datasource":{"type":"loki","uid":"loki"},"editorMode":"code","expr":"sum by (app, cluster) (count_over_time({app=~\".+\", cluster=~\".+\"} |= \"status:error\" |= \"limiting requests, excess:\" [5m])) > 1000","hide":false,"intervalMs":1000,"maxDataPoints":43200,"queryType":"instant","refId":"A"},"queryType":"instant","refId":"A","relativeTimeRange":{"from":600,"to":0}}],"execErrState":"KeepLast","for":"5m","isPaused":false,"labels":{},"noDataState":"OK","notification_settings":{"receiver":"Slack"},"title":"Nginx is logging excessive \" limiting requests, excess:\"","uid":"fdwbeuftc7400c"}]}]` | Groups of alerting rules. |
| lgtm.grafana.alerting."rules.yaml".groups[0].folder | string | `"Alerts"` | Folder where the alerts will be placed in Grafana. |
| lgtm.grafana.alerting."rules.yaml".groups[0].interval | string | `"5m"` | Interval at which the alert rules are evaluated. |
| lgtm.grafana.alerting."rules.yaml".groups[0].name | string | `"Alerts"` | Name of the alert group. |
| lgtm.grafana.alerting."rules.yaml".groups[0].rules | list | `[{"annotations":{"summary":"Alert: HTTP 500 errors detected in the environment: {{`{{ $labels.clusters }}`}}"},"condition":"A","data":[{"datasourceUid":"loki","model":{"datasource":{"type":"loki","uid":"loki"},"editorMode":"code","expr":"sum by (cluster) (count_over_time({cluster=~\".+\"} | json | http_status_code=\"500\" [1h])) > 0","hide":false,"intervalMs":1000,"maxDataPoints":43200,"queryType":"instant","refId":"A"},"queryType":"instant","refId":"A","relativeTimeRange":{"from":600,"to":0}}],"execErrState":"KeepLast","for":"5m","isPaused":false,"labels":{},"noDataState":"OK","notification_settings":{"receiver":"Slack"},"title":"HTTP 500 errors detected","uid":"edwb8zgcvq96oc"},{"annotations":{"description":"Error in usersync job detected in cluster {{`{{ $labels.clusters }}`}}, namespace {{`{{ $labels.namespace }}`}}.","summary":"Error Logs Detected in Usersync Job"},"condition":"A","data":[{"datasourceUid":"loki","model":{"datasource":{"type":"loki","uid":"loki"},"editorMode":"code","expr":"sum by (cluster, namespace) (count_over_time({ app=\"gen3job\", job_name=~\"usersync-.*\"} |= \"ERROR - could not revoke policies from user `N/A`\" [5m])) > 1","hide":false,"intervalMs":1000,"maxDataPoints":43200,"queryType":"instant","refId":"A"},"queryType":"instant","refId":"A","relativeTimeRange":{"from":600,"to":0}}],"execErrState":"KeepLast","for":"5m","isPaused":false,"labels":{},"noDataState":"OK","notification_settings":{"receiver":"Slack"},"title":"Error Logs Detected in Usersync Job","uid":"adwb9vhb7irr4b"},{"annotations":{"description":"Panic detected in app {{`{{ $labels.app }}`}} within cluster {{`{{ $labels.clusters }}`}}.","summary":"Hatchery panic"},"condition":"A","data":[{"datasourceUid":"loki","model":{"datasource":{"type":"loki","uid":"loki"},"editorMode":"code","expr":"sum by (cluster) (count_over_time({app=\"hatchery\"} |= \"panic\" [5m])) > 1","hide":false,"intervalMs":1000,"maxDataPoints":43200,"queryType":"instant","refId":"A"},"queryType":"instant","refId":"A","relativeTimeRange":{"from":600,"to":0}}],"execErrState":"KeepLast","for":"5m","isPaused":false,"labels":{},"noDataState":"OK","notification_settings":{"receiver":"Slack"},"title":"Hatchery panic in {{`{{ env.name }}`}}","uid":"ddwbc12l6wc8wf"},{"annotations":{"description":"Detected 431 HTTP status codes in the logs within the last 5 minutes.","summary":"Http status code 431"},"condition":"A","data":[{"datasourceUid":"loki","model":{"datasource":{"type":"loki","uid":"loki"},"editorMode":"code","expr":"sum(count_over_time({cluster=~\".+\"} | json | http_status_code=\"431\" [5m])) >= 2","hide":false,"intervalMs":1000,"maxDataPoints":43200,"queryType":"instant","refId":"A"},"queryType":"instant","refId":"A","relativeTimeRange":{"from":600,"to":0}}],"execErrState":"KeepLast","for":"5m","isPaused":false,"labels":{},"noDataState":"OK","notification_settings":{"receiver":"Slack"},"title":"Http status code 431","uid":"cdwbcbphz1zb4a"},{"annotations":{"description":"High number of info status logs detected in the indexd service in cluster {{`{{ $labels.clusters }}`}}.","summary":"Indexd is getting an excessive amount of traffic"},"condition":"A","data":[{"datasourceUid":"loki","model":{"datasource":{"type":"loki","uid":"loki"},"editorMode":"code","expr":"sum by (cluster) (count_over_time({cluster=~\".+\", app=\"indexd\", status=\"info\"} [5m])) > 50000","hide":false,"intervalMs":1000,"maxDataPoints":43200,"queryType":"instant","refId":"A"},"queryType":"instant","refId":"A","relativeTimeRange":{"from":600,"to":0}}],"execErrState":"KeepLast","for":"5m","isPaused":false,"labels":{},"noDataState":"OK","notification_settings":{"receiver":"Slack"},"title":"Indexd is getting an excessive amount of traffic","uid":"bdwbck1lgwdfka"},{"annotations":{"description":"More than 10 errors detected in the karpenter namespace in cluster {{`{{ $labels.clusters }}`}} related to providerRef not found.","summary":"Karpenter Resource Mismatch"},"condition":"A","data":[{"datasourceUid":"loki","model":{"datasource":{"type":"loki","uid":"loki"},"editorMode":"code","expr":"sum by (cluster) (count_over_time({namespace=\"karpenter\", cluster=~\".+\"} |= \"ERROR\" |= \"not found\" |= \"getting providerRef\" [5m])) > 10\n","hide":false,"intervalMs":1000,"maxDataPoints":43200,"queryType":"instant","refId":"A"},"queryType":"instant","refId":"A","relativeTimeRange":{"from":600,"to":0}}],"execErrState":"KeepLast","for":"5m","isPaused":false,"labels":{},"noDataState":"OK","notification_settings":{"receiver":"Slack"},"title":"Karpenter Resource Mismatch","uid":"fdwbe5t439zpcd"},{"annotations":{"description":"More than 1000 \"limiting requests, excess\" errors detected in service {{`{{ $labels.app }}`}} (cluster: {{`{{ $labels.clusters }}`}}) within the last 5 minutes.","summary":"Nginx is logging excessive \" limiting requests, excess:\""},"condition":"A","data":[{"datasourceUid":"loki","model":{"datasource":{"type":"loki","uid":"loki"},"editorMode":"code","expr":"sum by (app, cluster) (count_over_time({app=~\".+\", cluster=~\".+\"} |= \"status:error\" |= \"limiting requests, excess:\" [5m])) > 1000","hide":false,"intervalMs":1000,"maxDataPoints":43200,"queryType":"instant","refId":"A"},"queryType":"instant","refId":"A","relativeTimeRange":{"from":600,"to":0}}],"execErrState":"KeepLast","for":"5m","isPaused":false,"labels":{},"noDataState":"OK","notification_settings":{"receiver":"Slack"},"title":"Nginx is logging excessive \" limiting requests, excess:\"","uid":"fdwbeuftc7400c"}]` | List of alerting rules to be defined (add specific rules here). |
| lgtm.grafana.dashboardProviders | map | `{"dashboardproviders.yaml":{"apiVersion":1,"providers":[{"disableDeletion":true,"editable":true,"folder":"Kubernetes","name":"grafana-dashboards-kubernetes","options":{"path":"/var/lib/grafana/dashboards/grafana-dashboards-kubernetes"},"orgId":1,"type":"file"}]}}` | Configuration for dashboard providers in Grafana. |
| lgtm.grafana.dashboardProviders."dashboardproviders.yaml".apiVersion | int | `1` | API version for dashboard provider configuration. |
| lgtm.grafana.dashboardProviders."dashboardproviders.yaml".providers | list | `[{"disableDeletion":true,"editable":true,"folder":"Kubernetes","name":"grafana-dashboards-kubernetes","options":{"path":"/var/lib/grafana/dashboards/grafana-dashboards-kubernetes"},"orgId":1,"type":"file"}]` | List of dashboard providers. |
| lgtm.grafana.dashboardProviders."dashboardproviders.yaml".providers[0].disableDeletion | bool | `true` | Prevent deletion of the provided dashboards. |
| lgtm.grafana.dashboardProviders."dashboardproviders.yaml".providers[0].editable | bool | `true` | Allow editing of the dashboards. |
| lgtm.grafana.dashboardProviders."dashboardproviders.yaml".providers[0].folder | string | `"Kubernetes"` | Folder where the dashboards will be placed in Grafana. |
| lgtm.grafana.dashboardProviders."dashboardproviders.yaml".providers[0].options | map | `{"path":"/var/lib/grafana/dashboards/grafana-dashboards-kubernetes"}` | Options for the dashboard provider. |
| lgtm.grafana.dashboardProviders."dashboardproviders.yaml".providers[0].options.path | string | `"/var/lib/grafana/dashboards/grafana-dashboards-kubernetes"` | Path to the dashboard files. |
| lgtm.grafana.dashboardProviders."dashboardproviders.yaml".providers[0].orgId | int | `1` | Organization ID in Grafana. |
| lgtm.grafana.dashboardProviders."dashboardproviders.yaml".providers[0].type | string | `"file"` | Type of dashboard provider, usually 'file'. |
| lgtm.grafana.dashboards | map | `{"grafana-dashboards-kubernetes":{"k8s-system-api-server":{"token":"","url":"https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-system-api-server.json"},"k8s-system-coredns":{"token":"","url":"https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-system-coredns.json"},"k8s-views-global":{"token":"","url":"https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-views-global.json"},"k8s-views-namespaces":{"token":"","url":"https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-views-namespaces.json"},"k8s-views-nodes":{"token":"","url":"https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-views-nodes.json"},"k8s-views-pods":{"token":"","url":"https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-views-pods.json"}}}` | Dashboards configuration. URLs to fetch specific Kubernetes-related Grafana dashboards. Gen3 specific dashboards can be found here. https://github.com/uc-cdis/grafana-dashboards |
| lgtm.grafana.dashboards.grafana-dashboards-kubernetes.k8s-system-api-server.token | string | `""` | Authentication token for accessing the dashboard URL (optional). |
| lgtm.grafana.dashboards.grafana-dashboards-kubernetes.k8s-system-api-server.url | string | `"https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-system-api-server.json"` | URL to the dashboard JSON file for the Kubernetes API server. |
| lgtm.grafana.dashboards.grafana-dashboards-kubernetes.k8s-system-coredns.url | string | `"https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-system-coredns.json"` | URL to the dashboard JSON file for CoreDNS in Kubernetes. |
| lgtm.grafana.dashboards.grafana-dashboards-kubernetes.k8s-views-global.url | string | `"https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-views-global.json"` | URL to the dashboard JSON file for global views in Kubernetes. |
| lgtm.grafana.dashboards.grafana-dashboards-kubernetes.k8s-views-namespaces.url | string | `"https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-views-namespaces.json"` | URL to the dashboard JSON file for Kubernetes namespace views. |
| lgtm.grafana.dashboards.grafana-dashboards-kubernetes.k8s-views-nodes.url | string | `"https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-views-nodes.json"` | URL to the dashboard JSON file for Kubernetes node views. |
| lgtm.grafana.dashboards.grafana-dashboards-kubernetes.k8s-views-pods.url | string | `"https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-views-pods.json"` | URL to the dashboard JSON file for Kubernetes pod views. |
| lgtm.grafana.downloadDashboardsImage | map | `{"registry":"quay.io/curl","repository":"curl","tag":"8.8.0"}` | Image used to download Grafana dashboards. |
| lgtm.grafana.downloadDashboardsImage.registry | string | `"quay.io/curl"` | Container image registry for the dashboard download image. |
| lgtm.grafana.downloadDashboardsImage.repository | string | `"curl"` | Repository for the curl image. |
| lgtm.grafana.downloadDashboardsImage.tag | string | `"8.8.0"` | Tag for the curl image version. |
| lgtm.grafana.enabled | bool | `true` | Deploy Grafana if enabled. See [upstream readme](https://github.com/grafana/helm-charts/tree/main/charts/grafana#configuration) for full values reference. |
| lgtm.grafana.env | map | `{"GF_SERVER_ROOT_URL":"https://grafana.example.com"}` | Environment variables for Grafana. |
| lgtm.grafana.env.GF_SERVER_ROOT_URL | string | `"https://grafana.example.com"` | Root URL configuration for the Grafana server. |
| lgtm.grafana.envFromSecret | string | `nil` | Reference a secret for environment variables. |
| lgtm.grafana.image | map | `{"pullPolicy":"Always","registry":"quay.io/cdis","repository":"grafana","tag":"master"}` | Image configuration for Grafana. |
| lgtm.grafana.image.pullPolicy | string | `"Always"` | Pull policy for the Grafana image (e.g., 'Always'). |
| lgtm.grafana.image.registry | string | `"quay.io/cdis"` | Container image registry for Grafana. |
| lgtm.grafana.image.repository | string | `"grafana"` | Repository for the Grafana image. |
| lgtm.grafana.image.tag | string | `"master"` | Tag for the Grafana image version. |
| lgtm.grafana.ingress.annotations | map | `{}` | Annotations for Grafana ingress. |
| lgtm.grafana.ingress.enabled | bool | `true` | Enable or disable ingress for Grafana. |
| lgtm.grafana.ingress.hosts | list | `["grafana.example.com"]` | Hostname(s) for Grafana ingress. |
| lgtm.grafana.ingress.ingressClassName | string | `"alb"` | Ingress class name to be used (e.g., 'alb' for AWS Application Load Balancer). |
| lgtm.grafana.ingress.tls[0] | list | `{"secretName":null}` | TLS configuration for the ingress. Reference to a secret that contains the TLS certificate. |
| lgtm.grafana.initChownData | map | `{"image":{"registry":"quay.io/cdis","repository":"busybox","tag":"1.32.0"}}` | Init container to chown data directories for Grafana. |
| lgtm.grafana.initChownData.image.registry | string | `"quay.io/cdis"` | Container image registry for the init container. |
| lgtm.grafana.initChownData.image.repository | string | `"busybox"` | Repository for the busybox image. |
| lgtm.grafana.initChownData.image.tag | string | `"1.32.0"` | Tag for the busybox image version. |
| lgtm.grafana.persistence | map | `{"enabled":true}` | Persistence configuration for Grafana. |
| lgtm.grafana.persistence.enabled | bool | `true` | Enable or disable persistence for Grafana data. |
| lgtm.loki.distributor | map | `{"affinity":"nodeAffinity:\n  requiredDuringSchedulingIgnoredDuringExecution:\n    nodeSelectorTerms:\n    - matchExpressions:\n      # -- (string) Node label key for affinity. Ensures pods are scheduled on nodes in the specified zone.\n      - key: topology.kubernetes.io/zone\n        # -- (string) Operator to apply to the node selector. 'In' means the node must match one of the values.\n        operator: In\n        # -- (list) List of values for the node selector, representing allowed zones.\n        values:\n        - us-east-1a\n","maxUnavailable":2,"replicas":3,"resources":{"limits":{"memory":"6Gi"},"requests":{"cpu":2,"memory":"4Gi"}}}` | Scaling and configuring loki distributor. |
| lgtm.loki.distributor.affinity | map | `"nodeAffinity:\n  requiredDuringSchedulingIgnoredDuringExecution:\n    nodeSelectorTerms:\n    - matchExpressions:\n      # -- (string) Node label key for affinity. Ensures pods are scheduled on nodes in the specified zone.\n      - key: topology.kubernetes.io/zone\n        # -- (string) Operator to apply to the node selector. 'In' means the node must match one of the values.\n        operator: In\n        # -- (list) List of values for the node selector, representing allowed zones.\n        values:\n        - us-east-1a\n"` | Affinity rules for scheduling distributor pods.  Passed in as a multiline string. |
| lgtm.loki.distributor.maxUnavailable | int | `2` | Maximum number of unavailable replicas allowed during an update. |
| lgtm.loki.distributor.replicas | int | `3` | Number of replicas for the distributor component. Determines how many instances to run. |
| lgtm.loki.distributor.resources.limits | map | `{"memory":"6Gi"}` | Resource limits for the distributor component. |
| lgtm.loki.distributor.resources.limits.memory | string | `"6Gi"` | Memory limit for the distributor pods. |
| lgtm.loki.distributor.resources.requests | map | `{"cpu":2,"memory":"4Gi"}` | Resource requests for the distributor component. |
| lgtm.loki.distributor.resources.requests.cpu | string | `2` | CPU request for the distributor pods. Determines how much CPU is guaranteed for the pod. |
| lgtm.loki.distributor.resources.requests.memory | string | `"4Gi"` | Memory request for the distributor pods. Determines how much memory is guaranteed for the pod. |
| lgtm.loki.gateway.affinity | string | `"nodeAffinity:\n  requiredDuringSchedulingIgnoredDuringExecution:\n    nodeSelectorTerms:\n    - matchExpressions:\n      - key: topology.kubernetes.io/zone\n        operator: In\n        values:\n        - us-east-1a\n"` | Affinity rules for scheduling gateway pods. Passed in as a multiline string. |
| lgtm.loki.gateway.ingress.annotations | object | `{}` |  |
| lgtm.loki.gateway.ingress.enabled | bool | `true` | Enable or disable loki ingress. |
| lgtm.loki.gateway.ingress.hosts | list | `[{"host":"loki.example.com","paths":[{"path":"/","pathType":"Prefix"}]}]` | Hosts for loki ingress. |
| lgtm.loki.gateway.ingress.hosts[0] | string | `{"host":"loki.example.com","paths":[{"path":"/","pathType":"Prefix"}]}` | Hostname for loki ingress. |
| lgtm.loki.gateway.ingress.ingressClassName | string | `"alb"` | Class name for ingress. |
| lgtm.loki.ingester | map | `{"affinity":"nodeAffinity:\n  requiredDuringSchedulingIgnoredDuringExecution:\n    nodeSelectorTerms:\n    - matchExpressions:\n      # -- (string) Node label key for affinity. Ensures pods are scheduled on nodes in the specified zone.\n      - key: topology.kubernetes.io/zone\n        # -- (string) Operator to apply to the node selector. 'In' means the node must match one of the values.\n        operator: In\n        # -- (list) List of values for the node selector, representing allowed zones.\n        values:\n        - us-east-1a\n","maxUnavailable":2,"persistentVolume":{"size":"50Gi"},"replicas":3,"resources":{"limits":{"memory":"12Gi"},"requests":{"cpu":3.5,"memory":"8Gi"}}}` | Scaling and configuring loki ingester.  Passed in as a multiline string. |
| lgtm.loki.ingester.affinity | map | `"nodeAffinity:\n  requiredDuringSchedulingIgnoredDuringExecution:\n    nodeSelectorTerms:\n    - matchExpressions:\n      # -- (string) Node label key for affinity. Ensures pods are scheduled on nodes in the specified zone.\n      - key: topology.kubernetes.io/zone\n        # -- (string) Operator to apply to the node selector. 'In' means the node must match one of the values.\n        operator: In\n        # -- (list) List of values for the node selector, representing allowed zones.\n        values:\n        - us-east-1a\n"` | Affinity rules for scheduling ingester pods. |
| lgtm.loki.ingester.maxUnavailable | int | `2` | Maximum number of unavailable replicas allowed during an update. |
| lgtm.loki.ingester.persistentVolume | map | `{"size":"50Gi"}` | Persistent volume configuration for the ingester component. |
| lgtm.loki.ingester.persistentVolume.size | string | `"50Gi"` | Size of the persistent volume to be used by the ingester. |
| lgtm.loki.ingester.replicas | int | `3` | Number of replicas for the ingester component. Determines how many instances to run. |
| lgtm.loki.ingester.resources.limits | map | `{"memory":"12Gi"}` | Resource limits for the ingester component. |
| lgtm.loki.ingester.resources.limits.memory | string | `"12Gi"` | Memory limit for the ingester pods. |
| lgtm.loki.ingester.resources.requests | map | `{"cpu":3.5,"memory":"8Gi"}` | Resource requests for the ingester component. |
| lgtm.loki.ingester.resources.requests.cpu | string | `3.5` | CPU request for the ingester pods. Determines how much CPU is guaranteed for the pod. |
| lgtm.loki.ingester.resources.requests.memory | string | `"8Gi"` | Memory request for the ingester pods. Determines how much memory is guaranteed for the pod. |
| lgtm.loki.loki | map | `{"image":{"registry":"quay.io/cdis","repository":"loki","tag":"master"},"schemaConfig":{"configs":[{"from":"2024-04-01","index":{"period":"24h","prefix":"loki_index_"},"object_store":"s3","schema":"v13","store":"tsdb"}]},"structuredConfig":{"common":{"path_prefix":"/var/loki","storage":{"filesystem":null,"s3":{"region":"us-east-1"}}},"limits_config":{"max_entries_limit_per_query":100000000,"max_query_series":30000,"max_streams_per_user":100000},"server":{"log_level":"debug"}}}` | Loki configuration. |
| lgtm.loki.loki.image | map | `{"registry":"quay.io/cdis","repository":"loki","tag":"master"}` | Loki image details. |
| lgtm.loki.loki.image.registry | string | `"quay.io/cdis"` | Container image registry for Loki. |
| lgtm.loki.loki.image.repository | string | `"loki"` | Repository for the Loki image. |
| lgtm.loki.loki.image.tag | string | `"master"` | Tag for the Loki image version. |
| lgtm.loki.loki.schemaConfig | map | `{"configs":[{"from":"2024-04-01","index":{"period":"24h","prefix":"loki_index_"},"object_store":"s3","schema":"v13","store":"tsdb"}]}` | Schema configuration for Loki. |
| lgtm.loki.loki.schemaConfig.configs[0].index | map | `{"period":"24h","prefix":"loki_index_"}` | Index configuration for Loki. |
| lgtm.loki.loki.schemaConfig.configs[0].index.period | string | `"24h"` | Index rotation period for Loki, in hours. |
| lgtm.loki.loki.schemaConfig.configs[0].index.prefix | string | `"loki_index_"` | Prefix for the Loki index. |
| lgtm.loki.loki.schemaConfig.configs[0].object_store | string | `"s3"` | Object store for Loki data (e.g., S3). |
| lgtm.loki.loki.schemaConfig.configs[0].schema | string | `"v13"` | Schema version for Loki. |
| lgtm.loki.loki.schemaConfig.configs[0].store | string | `"tsdb"` | Storage engine used by Loki. |
| lgtm.loki.loki.structuredConfig | map | `{"common":{"path_prefix":"/var/loki","storage":{"filesystem":null,"s3":{"region":"us-east-1"}}},"limits_config":{"max_entries_limit_per_query":100000000,"max_query_series":30000,"max_streams_per_user":100000},"server":{"log_level":"debug"}}` | Structured configuration settings for Loki. |
| lgtm.loki.loki.structuredConfig.common.path_prefix | string | `"/var/loki"` | Path prefix where Loki stores data. |
| lgtm.loki.loki.structuredConfig.common.storage.filesystem | null | `nil` | Filesystem storage is disabled. |
| lgtm.loki.loki.structuredConfig.common.storage.s3.region | string | `"us-east-1"` | AWS region for S3 storage. |
| lgtm.loki.loki.structuredConfig.limits_config.max_entries_limit_per_query | int | `100000000` | Maximum number of log entries per query. |
| lgtm.loki.loki.structuredConfig.limits_config.max_query_series | int | `30000` | Maximum number of series that can be queried at once. |
| lgtm.loki.loki.structuredConfig.limits_config.max_streams_per_user | int | `100000` | Maximum number of streams a single user can have. |
| lgtm.loki.loki.structuredConfig.server.log_level | string | `"debug"` | Log level for Loki server. Options include 'info', 'debug', etc. |
| lgtm.loki.persistence | map | `{"enabled":true}` | Persistence settings for loki. |
| lgtm.loki.persistence.enabled | bool | `true` | Enable or disable persistence. |
| lgtm.loki.querier | map | `{"affinity":"nodeAffinity:\n  requiredDuringSchedulingIgnoredDuringExecution:\n    nodeSelectorTerms:\n    - matchExpressions:\n      # -- (string) Node label key for affinity. Ensures pods are scheduled on nodes in the specified zone.\n      - key: topology.kubernetes.io/zone\n        # -- (string) Operator to apply to the node selector. 'In' means the node must match one of the values.\n        operator: In\n        # -- (list) List of values for the node selector, representing allowed zones.\n        values:\n        - us-east-1a\n","resources":{"limits":{"memory":"6Gi"},"requests":{"cpu":2,"memory":"4Gi"}}}` | Scaling and configuring loki querier. |
| lgtm.loki.querier.affinity | string | `"nodeAffinity:\n  requiredDuringSchedulingIgnoredDuringExecution:\n    nodeSelectorTerms:\n    - matchExpressions:\n      # -- (string) Node label key for affinity. Ensures pods are scheduled on nodes in the specified zone.\n      - key: topology.kubernetes.io/zone\n        # -- (string) Operator to apply to the node selector. 'In' means the node must match one of the values.\n        operator: In\n        # -- (list) List of values for the node selector, representing allowed zones.\n        values:\n        - us-east-1a\n"` | Affinity rules for scheduling querier pods. Passed in as a multiline string. |
| lgtm.loki.querier.resources | map | `{"limits":{"memory":"6Gi"},"requests":{"cpu":2,"memory":"4Gi"}}` | Resource requests and limits for querier. |
| lgtm.loki.querier.resources.limits | map | `{"memory":"6Gi"}` | Resource limits for the querier component. |
| lgtm.loki.querier.resources.limits.memory | string | `"6Gi"` | Memory limit for the querier pods. |
| lgtm.loki.querier.resources.requests | map | `{"cpu":2,"memory":"4Gi"}` | Resource requests for the querier component. |
| lgtm.loki.querier.resources.requests.cpu | string | `2` | CPU request for the querier pods. Determines how much CPU is guaranteed for the pod. |
| lgtm.loki.querier.resources.requests.memory | string | `"4Gi"` | Memory request for the querier pods. Determines how much memory is guaranteed for the pod. |
| lgtm.loki.queryFrontend | map | `{"affinity":"nodeAffinity:\n  requiredDuringSchedulingIgnoredDuringExecution:\n    nodeSelectorTerms:\n    - matchExpressions:\n      # -- (string) Node label key for affinity. Ensures pods are scheduled on nodes in the specified zone.\n      - key: topology.kubernetes.io/zone\n        # -- (string) Operator to apply to the node selector. 'In' means the node must match one of the values.\n        operator: In\n        # -- (list) List of values for the node selector, representing allowed zones.\n        values:\n        - us-east-1a\n","resources":{"limits":{"memory":"6Gi"},"requests":{"cpu":2,"memory":"4Gi"}}}` | Scaling and configuring loki queryFrontend. |
| lgtm.loki.queryFrontend.affinity | map | `"nodeAffinity:\n  requiredDuringSchedulingIgnoredDuringExecution:\n    nodeSelectorTerms:\n    - matchExpressions:\n      # -- (string) Node label key for affinity. Ensures pods are scheduled on nodes in the specified zone.\n      - key: topology.kubernetes.io/zone\n        # -- (string) Operator to apply to the node selector. 'In' means the node must match one of the values.\n        operator: In\n        # -- (list) List of values for the node selector, representing allowed zones.\n        values:\n        - us-east-1a\n"` | Affinity rules for scheduling queryFrontend pods.  Passed in as a multiline string. |
| lgtm.loki.queryFrontend.resources | map | `{"limits":{"memory":"6Gi"},"requests":{"cpu":2,"memory":"4Gi"}}` | Resource requests and limits for queryFrontend. |
| lgtm.loki.queryFrontend.resources.limits | map | `{"memory":"6Gi"}` | Resource limits for the queryFrontend component. |
| lgtm.loki.queryFrontend.resources.limits.memory | string | `"6Gi"` | Memory limit for the queryFrontend pods. |
| lgtm.loki.queryFrontend.resources.requests | map | `{"cpu":2,"memory":"4Gi"}` | Resource requests for the queryFrontend component. |
| lgtm.loki.queryFrontend.resources.requests.cpu | string | `2` | CPU request for the queryFrontend pods. Determines how much CPU is guaranteed for the pod. |
| lgtm.loki.queryFrontend.resources.requests.memory | string | `"4Gi"` | Memory request for the queryFrontend pods. Determines how much memory is guaranteed for the pod. |
| lgtm.loki.serviceAccount | string | `{"name":"observability"}` | Service account configuration for loki. |
| lgtm.loki.serviceAccount.name | string | `"observability"` | Service account to use (will be created by default via this helm chart). |
| lgtm.mimir.alertmanager.affinity | map | `{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchExpressions":[{"key":"topology.kubernetes.io/zone","operator":"In","values":["us-east-1a"]}]}]}}}` | Affinity rules for scheduling Alertmanager pods. |
| lgtm.mimir.alertmanager.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0] | string | `{"key":"topology.kubernetes.io/zone","operator":"In","values":["us-east-1a"]}` | Node label key for affinity. Ensures pods are scheduled on nodes in the specified zone. |
| lgtm.mimir.alertmanager.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0].operator | string | `"In"` | Operator to apply to the node selector. 'In' means the node must match one of the values. |
| lgtm.mimir.alertmanager.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0].values | list | `["us-east-1a"]` | List of values for the node selector, representing allowed zones. |
| lgtm.mimir.alertmanager.persistentVolume | map | `{"enabled":true}` | Configuration for persistent volume in Alertmanager. |
| lgtm.mimir.alertmanager.persistentVolume.enabled | bool | `true` | Enable or disable the persistent volume for Alertmanager. Set to 'true' to enable, 'false' to disable. |
| lgtm.mimir.alertmanager.replicas | int | `3` | Number of replicas for Alertmanager. Determines how many instances of Alertmanager to run. |
| lgtm.mimir.alertmanager.resources.limits | map | `{"memory":"2Gi"}` | Resource limits for Alertmanager pods. |
| lgtm.mimir.alertmanager.resources.limits.memory | string | `"2Gi"` | Memory limit for Alertmanager pods. |
| lgtm.mimir.alertmanager.resources.requests | map | `{"cpu":1,"memory":"1Gi"}` | Resource requests for Alertmanager pods. |
| lgtm.mimir.alertmanager.resources.requests.cpu | string | `1` | CPU request for Alertmanager pods. Determines how much CPU is guaranteed for the pod. |
| lgtm.mimir.alertmanager.resources.requests.memory | string | `"1Gi"` | Memory request for Alertmanager pods. Determines how much memory is guaranteed for the pod. |
| lgtm.mimir.alertmanager.statefulSet | map | `{"enabled":true}` | Configuration for deploying Alertmanager as a StatefulSet. |
| lgtm.mimir.alertmanager.statefulSet.enabled | bool | `true` | Enable or disable the StatefulSet deployment for Alertmanager. Set to 'true' to enable, 'false' to disable. |
| lgtm.mimir.compactor.affinity | map | `{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchExpressions":[{"key":"topology.kubernetes.io/zone","operator":"In","values":["us-east-1a"]}]}]}}}` | Affinity rules for scheduling compactor pods. |
| lgtm.mimir.compactor.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0] | string | `{"key":"topology.kubernetes.io/zone","operator":"In","values":["us-east-1a"]}` | Node label key for affinity. Ensures pods are scheduled on nodes in the specified zone. |
| lgtm.mimir.compactor.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0].operator | string | `"In"` | Operator to apply to the node selector. 'In' means the node must match one of the values. |
| lgtm.mimir.compactor.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0].values | list | `["us-east-1a"]` | List of values for the node selector, representing allowed zones. |
| lgtm.mimir.compactor.persistentVolume | map | `{"size":"50Gi"}` | Persistent volume configuration for the compactor component. |
| lgtm.mimir.compactor.persistentVolume.size | string | `"50Gi"` | Size of the persistent volume to be used by the compactor. |
| lgtm.mimir.compactor.resources.limits | map | `{"memory":"3Gi"}` | Resource limits for the compactor component. |
| lgtm.mimir.compactor.resources.limits.memory | string | `"3Gi"` | Memory limit for the compactor pods. |
| lgtm.mimir.compactor.resources.requests | map | `{"cpu":1,"memory":"2Gi"}` | Resource requests for the compactor component. |
| lgtm.mimir.compactor.resources.requests.cpu | string | `1` | CPU request for the compactor pods. Determines how much CPU is guaranteed for the pod. |
| lgtm.mimir.compactor.resources.requests.memory | string | `"2Gi"` | Memory request for the compactor pods. Determines how much memory is guaranteed for the pod. |
| lgtm.mimir.distributor.affinity | map | `{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchExpressions":[{"key":"topology.kubernetes.io/zone","operator":"In","values":["us-east-1a"]}]}]}}}` | Affinity rules for scheduling distributor pods. |
| lgtm.mimir.distributor.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0] | string | `{"key":"topology.kubernetes.io/zone","operator":"In","values":["us-east-1a"]}` | Node label key for affinity. Ensures pods are scheduled on nodes in the specified zone. |
| lgtm.mimir.distributor.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0].operator | string | `"In"` | Operator to apply to the node selector. 'In' means the node must match one of the values. |
| lgtm.mimir.distributor.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0].values | list | `["us-east-1a"]` | List of values for the node selector, representing allowed zones. |
| lgtm.mimir.distributor.replicas | int | `3` | Number of replicas for the distributor component. Determines how many instances to run. |
| lgtm.mimir.distributor.resources.limits | map | `{"memory":"12Gi"}` | Resource limits for the distributor component. |
| lgtm.mimir.distributor.resources.limits.memory | string | `"12Gi"` | Memory limit for the distributor pods. |
| lgtm.mimir.distributor.resources.requests | map | `{"cpu":2,"memory":"8Gi"}` | Resource requests for the distributor component. |
| lgtm.mimir.distributor.resources.requests.cpu | string | `2` | CPU request for the distributor pods. Determines how much CPU is guaranteed for the pod. |
| lgtm.mimir.distributor.resources.requests.memory | string | `"8Gi"` | Memory request for the distributor pods. Determines how much memory is guaranteed for the pod. |
| lgtm.mimir.gateway.affinity | map | `{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchExpressions":[{"key":"topology.kubernetes.io/zone","operator":"In","values":["us-east-1a"]}]}]}}}` | Affinity rules for scheduling gateway pods. |
| lgtm.mimir.gateway.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0] | string | `{"key":"topology.kubernetes.io/zone","operator":"In","values":["us-east-1a"]}` | Node label key for affinity. Ensures pods are scheduled on nodes in the specified zone. |
| lgtm.mimir.gateway.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0].operator | string | `"In"` | Operator to apply to the node selector. 'In' means the node must match one of the values. |
| lgtm.mimir.gateway.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0].values | list | `["us-east-1a"]` | List of values for the node selector, representing allowed zones. |
| lgtm.mimir.gateway.replicas | int | `3` | Number of replicas for the gateway component. Determines how many instances to run. |
| lgtm.mimir.gateway.resources.limits | map | `{"memory":"731Mi"}` | Resource limits for the gateway component. |
| lgtm.mimir.gateway.resources.limits.memory | string | `"731Mi"` | Memory limit for the gateway pods. |
| lgtm.mimir.gateway.resources.requests | map | `{"cpu":1,"memory":"512Mi"}` | Resource requests for the gateway component. |
| lgtm.mimir.gateway.resources.requests.cpu | string | `1` | CPU request for the gateway pods. Determines how much CPU is guaranteed for the pod. |
| lgtm.mimir.gateway.resources.requests.memory | string | `"512Mi"` | Memory request for the gateway pods. Determines how much memory is guaranteed for the pod. |
| lgtm.mimir.image | map | `{"repository":"quay.io/cdis/mimir","tag":"master"}` | Docker image information. |
| lgtm.mimir.image.repository | string | `"quay.io/cdis/mimir"` | The Docker image repository for mimir. |
| lgtm.mimir.ingester.affinity.nodeAffinity | map | `{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchExpressions":[{"key":"topology.kubernetes.io/zone","operator":"In","values":["us-east-1a"]}]}]}}` | Affinity rules for scheduling ingester pods. |
| lgtm.mimir.ingester.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0] | string | `{"key":"topology.kubernetes.io/zone","operator":"In","values":["us-east-1a"]}` | Node label key for affinity. Ensures pods are scheduled on nodes in the specified zone. |
| lgtm.mimir.ingester.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0].operator | string | `"In"` | Operator to apply to the node selector. 'In' means the node must match one of the values. |
| lgtm.mimir.ingester.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0].values | list | `["us-east-1a"]` | List of values for the node selector, representing allowed zones. |
| lgtm.mimir.ingester.persistentVolume | map | `{"size":"50Gi"}` | Persistent volume configuration for the ingester component. |
| lgtm.mimir.ingester.persistentVolume.size | string | `"50Gi"` | Size of the persistent volume to be used by the ingester. |
| lgtm.mimir.ingester.replicas | int | `5` | Number of replicas for the ingester component. Determines how many instances to run. |
| lgtm.mimir.ingester.resources.limits | map | `{"memory":"12Gi"}` | Resource limits for the ingester component. |
| lgtm.mimir.ingester.resources.limits.memory | string | `"12Gi"` | Memory limit for the ingester pods. |
| lgtm.mimir.ingester.resources.requests | map | `{"cpu":3.5,"memory":"8Gi"}` | Resource requests for the ingester component. |
| lgtm.mimir.ingester.resources.requests.cpu | string | `3.5` | CPU request for the ingester pods. Determines how much CPU is guaranteed for the pod. |
| lgtm.mimir.ingester.resources.requests.memory | string | `"8Gi"` | Memory request for the ingester pods. Determines how much memory is guaranteed for the pod. |
| lgtm.mimir.ingester.topologySpreadConstraints | map | `{}` | Topology spread constraints for the ingester component. Empty by default. |
| lgtm.mimir.ingester.zoneAwareReplication | map | `{"topologyKey":"kubernetes.io/hostname"}` | Zone-aware replication settings. Helps distribute data across zones. |
| lgtm.mimir.ingester.zoneAwareReplication.topologyKey | string | `"kubernetes.io/hostname"` | Topology key used for zone-aware replication. |
| lgtm.mimir.ingress.annotations | object | `{}` |  |
| lgtm.mimir.ingress.enabled | bool | `true` | Enable or disable mirmir ingress. |
| lgtm.mimir.ingress.hosts | list | `["mimir.example.com"]` | hostname for mimir ingress. |
| lgtm.mimir.ingress.ingressClassName | string | `"alb"` | Class name for ingress. |
| lgtm.mimir.ingress.paths | map | `{"query-frontend":[{"path":"/prometheus/api/v1/query"}]}` | Additional paths to add to the ingress. |
| lgtm.mimir.ingress.paths.query-frontend | list | `[{"path":"/prometheus/api/v1/query"}]` | Additional paths to add to the query frontend. |
| lgtm.mimir.mimir.structuredConfig | map | `{"alertmanager_storage":{"storage_prefix":"alertmanager"},"blocks_storage":{"storage_prefix":"blocks"},"common":{"storage":{"backend":"s3","s3":{"endpoint":"s3.us-east-1.amazonaws.com","region":"us-east-1"}}},"limits":{"ingestion_rate":10000000,"max_global_series_per_user":0},"query_scheduler":{"service_discovery_mode":"dns"},"ruler_storage":{"storage_prefix":"ruler"}}` | Structured configuration settings for mimir. |
| lgtm.mimir.mimir.structuredConfig.alertmanager_storage.storage_prefix | string | `"alertmanager"` | Prefix used for storing Alertmanager data. |
| lgtm.mimir.mimir.structuredConfig.blocks_storage.storage_prefix | string | `"blocks"` | Prefix used for storing blocks data. |
| lgtm.mimir.mimir.structuredConfig.common.storage.backend | string | `"s3"` | Backend storage configuration. For example, s3 for AWS S3 storage. |
| lgtm.mimir.mimir.structuredConfig.common.storage.s3.endpoint | string | `"s3.us-east-1.amazonaws.com"` | The S3 endpoint to use for storage. Ensure this matches your region. |
| lgtm.mimir.mimir.structuredConfig.common.storage.s3.region | string | `"us-east-1"` | AWS region where your S3 bucket is located. |
| lgtm.mimir.mimir.structuredConfig.limits.ingestion_rate | int | `10000000` | The rate limit for ingestion, measured in samples per second. |
| lgtm.mimir.mimir.structuredConfig.limits.max_global_series_per_user | int | `0` | Maximum number of global series allowed per user. Set to '0' for unlimited. |
| lgtm.mimir.mimir.structuredConfig.query_scheduler.service_discovery_mode | string | `"dns"` | Mode for service discovery in the query scheduler. Set to 'dns' for DNS-based service discovery. |
| lgtm.mimir.mimir.structuredConfig.ruler_storage.storage_prefix | string | `"ruler"` | Prefix used for storing ruler data. |
| lgtm.mimir.minio | map | `{"enabled":false}` | minio configuration. |
| lgtm.mimir.minio.enabled | bool | `false` | Enable or disable minio. |
| lgtm.mimir.nginx.affinity | string | `"nodeAffinity:\n  requiredDuringSchedulingIgnoredDuringExecution:\n    nodeSelectorTerms:\n    - matchExpressions:\n      # -- (string) Node label key for affinity. Ensures pods are scheduled on nodes in the specified zone.\n      - key: topology.kubernetes.io/zone\n        # -- (string) Operator to apply to the node selector. 'In' means the node must match one of the values.\n        operator: In\n        # -- (list) List of values for the node selector, representing allowed zones.\n        values:\n        - us-east-1a\n"` | Affinity rules for scheduling nginx pods. Passed in as a multiline string. |
| lgtm.mimir.nginx.image.registry | string | `"quay.io/nginx"` | Container image registry for nginx. |
| lgtm.mimir.nginx.image.repository | string | `"nginx-unprivileged"` | Repository for nginx unprivileged image. |
| lgtm.mimir.nginx.replicas | int | `3` | Number of replicas for the nginx component. Determines how many instances to run. |
| lgtm.mimir.nginx.resources.limits | map | `{"memory":"731Mi"}` | Resource limits for the nginx component. |
| lgtm.mimir.nginx.resources.limits.memory | string | `"731Mi"` | Memory limit for the nginx pods. |
| lgtm.mimir.nginx.resources.requests | map | `{"cpu":1,"memory":"512Mi"}` | Resource requests for the nginx component. |
| lgtm.mimir.nginx.resources.requests.cpu | string | `1` | CPU request for the nginx pods. Determines how much CPU is guaranteed for the pod. |
| lgtm.mimir.nginx.resources.requests.memory | string | `"512Mi"` | Memory request for the nginx pods. Determines how much memory is guaranteed for the pod. |
| lgtm.mimir.overrides_exporter.affinity | map | `{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchExpressions":[{"key":"topology.kubernetes.io/zone","operator":"In","values":["us-east-1a"]}]}]}}}` | Affinity rules for scheduling overrides_exporter pods. |
| lgtm.mimir.overrides_exporter.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0] | string | `{"key":"topology.kubernetes.io/zone","operator":"In","values":["us-east-1a"]}` | Node label key for affinity. Ensures pods are scheduled on nodes in the specified zone. |
| lgtm.mimir.overrides_exporter.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0].operator | string | `"In"` | Operator to apply to the node selector. 'In' means the node must match one of the values. |
| lgtm.mimir.overrides_exporter.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0].values | list | `["us-east-1a"]` | List of values for the node selector, representing allowed zones. |
| lgtm.mimir.overrides_exporter.replicas | int | `1` | Number of replicas for the overrides_exporter component. Determines how many instances to run. |
| lgtm.mimir.overrides_exporter.resources.limits | map | `{"memory":"128Mi"}` | Resource limits for the overrides_exporter component. |
| lgtm.mimir.overrides_exporter.resources.limits.memory | string | `"128Mi"` | Memory limit for the overrides_exporter pods. |
| lgtm.mimir.overrides_exporter.resources.requests | map | `{"cpu":"100m","memory":"128Mi"}` | Resource requests for the overrides_exporter component. |
| lgtm.mimir.overrides_exporter.resources.requests.cpu | string | `"100m"` | CPU request for the overrides_exporter pods. Determines how much CPU is guaranteed for the pod. |
| lgtm.mimir.overrides_exporter.resources.requests.memory | string | `"128Mi"` | Memory request for the overrides_exporter pods. Determines how much memory is guaranteed for the pod. |
| lgtm.mimir.querier.affinity | map | `{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchExpressions":[{"key":"topology.kubernetes.io/zone","operator":"In","values":["us-east-1a"]}]}]}}}` | Affinity rules for scheduling querier pods. |
| lgtm.mimir.querier.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0] | string | `{"key":"topology.kubernetes.io/zone","operator":"In","values":["us-east-1a"]}` | Node label key for affinity. Ensures pods are scheduled on nodes in the specified zone. |
| lgtm.mimir.querier.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0].operator | string | `"In"` | Operator to apply to the node selector. 'In' means the node must match one of the values. |
| lgtm.mimir.querier.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0].values | list | `["us-east-1a"]` | List of values for the node selector, representing allowed zones. |
| lgtm.mimir.querier.replicas | int | `3` | Number of replicas for the querier component. Determines how many instances to run. |
| lgtm.mimir.querier.resources.limits | map | `{"memory":"8Gi"}` | Resource limits for the querier component. |
| lgtm.mimir.querier.resources.limits.memory | string | `"8Gi"` | Memory limit for the querier pods. |
| lgtm.mimir.querier.resources.requests | map | `{"cpu":2,"memory":"6Gi"}` | Resource requests for the querier component. |
| lgtm.mimir.querier.resources.requests.cpu | string | `2` | CPU request for the querier pods. Determines how much CPU is guaranteed for the pod. |
| lgtm.mimir.querier.resources.requests.memory | string | `"6Gi"` | Memory request for the querier pods. Determines how much memory is guaranteed for the pod. |
| lgtm.mimir.query_frontend.affinity | map | `{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchExpressions":[{"key":"topology.kubernetes.io/zone","operator":"In","values":["us-east-1a"]}]}]}}}` | Affinity rules for scheduling query_frontend pods. |
| lgtm.mimir.query_frontend.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0] | string | `{"key":"topology.kubernetes.io/zone","operator":"In","values":["us-east-1a"]}` | Node label key for affinity. Ensures pods are scheduled on nodes in the specified zone. |
| lgtm.mimir.query_frontend.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0].operator | string | `"In"` | Operator to apply to the node selector. 'In' means the node must match one of the values. |
| lgtm.mimir.query_frontend.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0].values | list | `["us-east-1a"]` | List of values for the node selector, representing allowed zones. |
| lgtm.mimir.query_frontend.replicas | int | `2` | Number of replicas for the query_frontend component. Determines how many instances to run. |
| lgtm.mimir.query_frontend.resources.limits | map | `{"memory":"3Gi"}` | Resource limits for the query_frontend component. |
| lgtm.mimir.query_frontend.resources.limits.memory | string | `"3Gi"` | Memory limit for the query_frontend pods. |
| lgtm.mimir.query_frontend.resources.requests | map | `{"cpu":2,"memory":"2Gi"}` | Resource requests for the query_frontend component. |
| lgtm.mimir.query_frontend.resources.requests.cpu | string | `2` | CPU request for the query_frontend pods. Determines how much CPU is guaranteed for the pod. |
| lgtm.mimir.query_frontend.resources.requests.memory | string | `"2Gi"` | Memory request for the query_frontend pods. Determines how much memory is guaranteed for the pod. |
| lgtm.mimir.query_scheduler.affinity | map | `{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchExpressions":[{"key":"topology.kubernetes.io/zone","operator":"In","values":["us-east-1a"]}]}]}}}` | Affinity rules for scheduling query_scheduler pods. |
| lgtm.mimir.query_scheduler.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0] | string | `{"key":"topology.kubernetes.io/zone","operator":"In","values":["us-east-1a"]}` | Node label key for affinity. Ensures pods are scheduled on nodes in the specified zone. |
| lgtm.mimir.query_scheduler.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0].operator | string | `"In"` | Operator to apply to the node selector. 'In' means the node must match one of the values. |
| lgtm.mimir.query_scheduler.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0].values | list | `["us-east-1a"]` | List of values for the node selector, representing allowed zones. |
| lgtm.mimir.rollout_operator | map | `{"image":{"repository":"quay.io/cdis/rollout-operator","tag":"master"}}` | Rollout Operator configuration. |
| lgtm.mimir.rollout_operator.image | map | `{"repository":"quay.io/cdis/rollout-operator","tag":"master"}` | Docker image information. |
| lgtm.mimir.rollout_operator.image.repository | string | `"quay.io/cdis/rollout-operator"` | The Docker image repository for the rollout-operator. |
| lgtm.mimir.ruler.affinity | map | `{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchExpressions":[{"key":"topology.kubernetes.io/zone","operator":"In","values":["us-east-1a"]}]}]}}}` | Affinity rules for scheduling ruler pods. |
| lgtm.mimir.ruler.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0] | string | `{"key":"topology.kubernetes.io/zone","operator":"In","values":["us-east-1a"]}` | Node label key for affinity. Ensures pods are scheduled on nodes in the specified zone. |
| lgtm.mimir.ruler.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0].operator | string | `"In"` | Operator to apply to the node selector. 'In' means the node must match one of the values. |
| lgtm.mimir.ruler.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0].values | list | `["us-east-1a"]` | List of values for the node selector, representing allowed zones. |
| lgtm.mimir.ruler.replicas | int | `2` | Number of replicas for the ruler component. Determines how many instances to run. |
| lgtm.mimir.ruler.resources.limits | map | `{"memory":"5Gi"}` | Resource limits for the ruler component. |
| lgtm.mimir.ruler.resources.limits.memory | string | `"5Gi"` | Memory limit for the ruler pods. |
| lgtm.mimir.ruler.resources.requests | map | `{"cpu":1,"memory":"4Gi"}` | Resource requests for the ruler component. |
| lgtm.mimir.ruler.resources.requests.cpu | string | `1` | CPU request for the ruler pods. Determines how much CPU is guaranteed for the pod. |
| lgtm.mimir.ruler.resources.requests.memory | string | `"4Gi"` | Memory request for the ruler pods. Determines how much memory is guaranteed for the pod. |
| lgtm.mimir.serviceAccount.create | bool | `false` | Whether to create a service account or not. In case 'create' is false, do set 'name' to an existing service account name. The "observability" SA will be created by default via Helm. |
| lgtm.mimir.serviceAccount.name | string | `"observability"` | Override for the generated service account name. |
| lgtm.mimir.store_gateway.affinity | map | `{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchExpressions":[{"key":"topology.kubernetes.io/zone","operator":"In","values":["us-east-1a"]}]}]}}}` | Affinity rules for scheduling store_gateway pods. |
| lgtm.mimir.store_gateway.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0] | string | `{"key":"topology.kubernetes.io/zone","operator":"In","values":["us-east-1a"]}` | Node label key for affinity. Ensures pods are scheduled on nodes in the specified zone. |
| lgtm.mimir.store_gateway.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0].operator | string | `"In"` | Operator to apply to the node selector. 'In' means the node must match one of the values. |
| lgtm.mimir.store_gateway.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0].values | list | `["us-east-1a"]` | List of values for the node selector, representing allowed zones. |
| lgtm.mimir.store_gateway.persistentVolume | map | `{"size":"50Gi"}` | Persistent volume configuration for the store_gateway component. |
| lgtm.mimir.store_gateway.persistentVolume.size | string | `"50Gi"` | Size of the persistent volume to be used by the store_gateway. |
| lgtm.mimir.store_gateway.replicas | int | `2` | Number of replicas for the store_gateway component. Determines how many instances to run. |
| lgtm.mimir.store_gateway.resources.limits | map | `{"memory":"8Gi"}` | Resource limits for the store_gateway component. |
| lgtm.mimir.store_gateway.resources.limits.memory | string | `"8Gi"` | Memory limit for the store_gateway pods. |
| lgtm.mimir.store_gateway.resources.requests | map | `{"cpu":1,"memory":"6Gi"}` | Resource requests for the store_gateway component. |
| lgtm.mimir.store_gateway.resources.requests.cpu | string | `1` | CPU request for the store_gateway pods. Determines how much CPU is guaranteed for the pod. |
| lgtm.mimir.store_gateway.resources.requests.memory | string | `"6Gi"` | Memory request for the store_gateway pods. Determines how much memory is guaranteed for the pod. |
| lgtm.mimir.store_gateway.topologySpreadConstraints | map | `{}` | Topology spread constraints for the store_gateway component. Empty by default. |
| lgtm.mimir.store_gateway.zoneAwareReplication | map | `{"topologyKey":"kubernetes.io/hostname"}` | Zone-aware replication settings. Helps distribute data across zones. |
| lgtm.mimir.store_gateway.zoneAwareReplication.topologyKey | string | `"kubernetes.io/hostname"` | Topology key used for zone-aware replication. |
| lgtm.role.arn | string | `nil` | The arn of the aws role to associate with the service account that will be used for Loki and Mimir. Documentation on IRSA setup https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html |
| lgtm.tempo.enabled | bool | `false` | Enable or disable tempo. |
